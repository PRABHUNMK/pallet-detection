{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "421b406e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to C:\\Users\\Prabhu/.cache\\torch\\hub\\master.zip\n",
      "YOLOv5  2023-6-1 Python-3.10.4 torch-2.0.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7015519 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.7.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\calib3d\\src\\solvepnp.cpp:840: error: (-215:Assertion failed) ( (npoints >= 4) || (npoints == 3 && flags == SOLVEPNP_ITERATIVE && useExtrinsicGuess) || (npoints >= 3 && flags == SOLVEPNP_SQPNP) ) && npoints == std::max(ipoints.checkVector(2, CV_32F), ipoints.checkVector(2, CV_64F)) in function 'cv::solvePnPGeneric'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 44>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     86\u001b[0m image_points \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([[x_min, y_min],\n\u001b[0;32m     87\u001b[0m                          [x_min, y_max],\n\u001b[0;32m     88\u001b[0m                          [x_max, y_max],\n\u001b[0;32m     89\u001b[0m                          [x_max, y_min],\n\u001b[0;32m     90\u001b[0m                          [\u001b[38;5;28mint\u001b[39m((x_min \u001b[38;5;241m+\u001b[39m x_max) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m), \u001b[38;5;28mint\u001b[39m((y_min \u001b[38;5;241m+\u001b[39m y_max) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m)]])\n\u001b[0;32m     92\u001b[0m \u001b[38;5;66;03m# Perform PnP\u001b[39;00m\n\u001b[1;32m---> 93\u001b[0m success, rvec, tvec \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msolvePnP\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobject_points\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_points\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcamera_matrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdist_coeffs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;66;03m# Calculate tilt angle with respect to the camera\u001b[39;00m\n\u001b[0;32m     97\u001b[0m     _, _, yaw \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mRodrigues(rvec)[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.7.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\calib3d\\src\\solvepnp.cpp:840: error: (-215:Assertion failed) ( (npoints >= 4) || (npoints == 3 && flags == SOLVEPNP_ITERATIVE && useExtrinsicGuess) || (npoints >= 3 && flags == SOLVEPNP_SQPNP) ) && npoints == std::max(ipoints.checkVector(2, CV_32F), ipoints.checkVector(2, CV_64F)) in function 'cv::solvePnPGeneric'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import pyrealsense2 as rs\n",
    "import math\n",
    "\n",
    "# Load the YOLOv5 model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = torch.hub.load('ultralytics/yolov5', 'custom', 'best.pt', force_reload=True)\n",
    "model.to(device).eval()\n",
    "\n",
    "# Set camera parameters\n",
    "width, height = 640, 480  # Set the desired frame size\n",
    "fps = 30  # Set the desired frame rate\n",
    "\n",
    "# Initialize the RealSense camera\n",
    "pipeline = rs.pipeline()\n",
    "config = rs.config()\n",
    "config.enable_stream(rs.stream.depth, width, height, rs.format.z16, fps)\n",
    "config.enable_stream(rs.stream.color, width, height, rs.format.bgr8, fps)\n",
    "\n",
    "# Start the camera stream\n",
    "pipeline.start(config)\n",
    "\n",
    "# Create a pipeline profile to get the camera intrinsics\n",
    "profile = pipeline.get_active_profile()\n",
    "depth_profile = rs.video_stream_profile(profile.get_stream(rs.stream.depth))\n",
    "depth_intrinsics = depth_profile.get_intrinsics()\n",
    "\n",
    "# Retrieve camera calibration parameters\n",
    "fx = depth_intrinsics.fx\n",
    "fy = depth_intrinsics.fy\n",
    "cx = depth_intrinsics.ppx\n",
    "cy = depth_intrinsics.ppy\n",
    "k1, k2, p1, p2, k3 = depth_intrinsics.coeffs\n",
    "\n",
    "# Load camera matrix and distortion coefficients\n",
    "camera_matrix = np.array([[fx, 0, cx],\n",
    "                          [0, fy, cy],\n",
    "                          [0, 0, 1]])\n",
    "dist_coeffs = np.array([k1, k2, p1, p2, k3])\n",
    "\n",
    "# Object detection loop\n",
    "while True:\n",
    "    # Wait for a new frame\n",
    "    frames = pipeline.wait_for_frames()\n",
    "    depth_frame = frames.get_depth_frame()\n",
    "    color_frame = frames.get_color_frame()\n",
    "\n",
    "    # Convert the frames to numpy arrays\n",
    "    depth_image = np.asanyarray(depth_frame.get_data())\n",
    "    color_image = np.asanyarray(color_frame.get_data())\n",
    "\n",
    "    # Perform object detection on the color image\n",
    "    results = model(color_image)\n",
    "\n",
    "    # Display the results\n",
    "    for result in results.xyxy[0]:\n",
    "        if result is not None:\n",
    "            xyxy = result[:4].tolist()\n",
    "            conf = result[4].item()\n",
    "            cls = int(result[5].item())\n",
    "\n",
    "            # Only draw bounding box and label if confidence is greater than 0.5\n",
    "            if conf > 0.5:\n",
    "                # Convert 2D bounding box to 3D\n",
    "                x_min, y_min, x_max, y_max = xyxy\n",
    "\n",
    "                # Find the depth value at the center of the bounding box\n",
    "                x_center = int((x_min + x_max) / 2)\n",
    "                y_center = int((y_min + y_max) / 2)\n",
    "                depth_value = depth_frame.get_distance(x_center, y_center)\n",
    "\n",
    "                # Convert the depth value to 3D coordinates\n",
    "                depth_point = rs.rs2_deproject_pixel_to_point(depth_intrinsics, [x_center, y_center], depth_value)\n",
    "                x, y, z = depth_point\n",
    "\n",
    "                # Create the object points for PnP\n",
    "                object_points = np.array([[0, 0, 0],\n",
    "                                          [0, 1, 0],\n",
    "                                          [1, 1, 0],\n",
    "                                          [1, 0, 0],\n",
    "                                          [1, 1, 0]])\n",
    "\n",
    "                # Create the image points for PnP\n",
    "                image_points = np.array([[x_min, y_min],\n",
    "                                         [x_min, y_max],\n",
    "                                         [x_max, y_max],\n",
    "                                         [x_max, y_min],\n",
    "                                         [int((x_min + x_max) / 2), int((y_min + y_max) / 2)]])\n",
    "\n",
    "                # Perform PnP\n",
    "                success, rvec, tvec = cv2.solvePnP(object_points, image_points, camera_matrix, dist_coeffs)\n",
    "\n",
    "                if success:\n",
    "                    # Calculate tilt angle with respect to the camera\n",
    "                    _, _, yaw = cv2.Rodrigues(rvec)[0]\n",
    "                    tilt_angle = math.degrees(yaw)\n",
    "\n",
    "                    # Draw bounding box and label on the color image\n",
    "                    cv2.rectangle(color_image, (int(x_min), int(y_min)), (int(x_max), int(y_max)), (255, 0, 0), 2)\n",
    "                    cv2.putText(color_image, f'Tilt Angle: {tilt_angle:.2f} degrees', (int(x_min), int(y_min) - 10),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)\n",
    "\n",
    "    # Display the color image\n",
    "    cv2.imshow('Object Detection', color_image)\n",
    "\n",
    "    # Exit the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "# Stop the camera stream\n",
    "pipeline.stop()\n",
    "\n",
    "# Close all windows\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "175418f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to C:\\Users\\Prabhu/.cache\\torch\\hub\\master.zip\n",
      "YOLOv5  2023-6-2 Python-3.10.4 torch-2.0.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7015519 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n",
      "YOLOv5  2023-6-2 Python-3.10.4 torch-2.0.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7015519 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import pyrealsense2 as rs\n",
    "\n",
    "# Load the YOLOv5 model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = torch.hub.load('ultralytics/yolov5', 'custom', 'best.pt',force_reload=True)\n",
    "model = torch.hub.load('yolov5', 'custom', 'best.pt',source='local')\n",
    "model.to(device).eval()\n",
    "\n",
    "# Set camera parametersq\n",
    "width, height = 640, 480  # Set the desired frame size\n",
    "fps = 30  # Set the desired frame rate\n",
    "\n",
    "# Initialize the RealSense camera\n",
    "pipeline = rs.pipeline()\n",
    "config = rs.config()\n",
    "config.enable_stream(rs.stream.color, width, height, rs.format.bgr8, fps)\n",
    "\n",
    "# Start the camera stream\n",
    "pipeline.start(config)\n",
    "\n",
    "# Object detection loop\n",
    "while True:\n",
    "    # Wait for a new frame\n",
    "    frames = pipeline.wait_for_frames()\n",
    "    color_frame = frames.get_color_frame()\n",
    "    \n",
    "    # Convert the frame to a numpy array\n",
    "    frame = np.asanyarray(color_frame.get_data())\n",
    "\n",
    "    # Perform object detection\n",
    "    results = model(frame)\n",
    "\n",
    "    # Display the results\n",
    "    for result in results.xyxy[0]:\n",
    "        if result is not None:\n",
    "            xyxy = result[:4].tolist()\n",
    "            conf = result[4].item()\n",
    "            cls = int(result[5].item())\n",
    "\n",
    "            # Only draw bounding box and label if confidence is greater than 0.7\n",
    "            if conf > 0.5:\n",
    "                # Draw bounding box and label on the frame\n",
    "                cv2.rectangle(frame, (int(xyxy[0]), int(xyxy[1])), (int(xyxy[2]), int(xyxy[3])), (255, 0, 0), 2)\n",
    "                cv2.putText(frame, f'{cls}', (int(xyxy[0]), int(xyxy[1]) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('Object Detection', frame)\n",
    "\n",
    "    # Exit the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "# Stop the camera stream\n",
    "pipeline.stop()\n",
    "\n",
    "# Close all windows\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4a95b97b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to C:\\Users\\Prabhu/.cache\\torch\\hub\\master.zip\n",
      "YOLOv5  2023-6-1 Python-3.10.4 torch-2.0.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7015519 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import pyrealsense2 as rs\n",
    "import math\n",
    "\n",
    "# Load the YOLOv5 model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = torch.hub.load('ultralytics/yolov5', 'custom', 'best.pt', force_reload=True)\n",
    "model.to(device).eval()\n",
    "\n",
    "# Set camera parameters\n",
    "width, height = 640, 480  # Set the desired frame size\n",
    "fps = 30  # Set the desired frame rate\n",
    "\n",
    "# Initialize the RealSense camera\n",
    "pipeline = rs.pipeline()\n",
    "config = rs.config()\n",
    "config.enable_stream(rs.stream.depth, width, height, rs.format.z16, fps)\n",
    "config.enable_stream(rs.stream.color, width, height, rs.format.bgr8, fps)\n",
    "\n",
    "# Start the camera stream\n",
    "pipeline.start(config)\n",
    "\n",
    "# Create a pipeline profile to get the camera intrinsics\n",
    "profile = pipeline.get_active_profile()\n",
    "depth_profile = rs.video_stream_profile(profile.get_stream(rs.stream.depth))\n",
    "depth_intrinsics = depth_profile.get_intrinsics()\n",
    "\n",
    "# Retrieve camera calibration parameters\n",
    "fx = depth_intrinsics.fx\n",
    "fy = depth_intrinsics.fy\n",
    "cx = depth_intrinsics.ppx\n",
    "cy = depth_intrinsics.ppy\n",
    "k1, k2, p1, p2, k3 = depth_intrinsics.coeffs\n",
    "\n",
    "# Load camera matrix and distortion coefficients\n",
    "camera_matrix = np.array([[fx, 0, cx],\n",
    "                          [0, fy, cy],\n",
    "                          [0, 0, 1]])\n",
    "dist_coeffs = np.array([k1, k2, p1, p2, k3])\n",
    "\n",
    "# Object detection loop\n",
    "while True:\n",
    "    # Wait for a new frame\n",
    "    frames = pipeline.wait_for_frames()\n",
    "    depth_frame = frames.get_depth_frame()\n",
    "    color_frame = frames.get_color_frame()\n",
    "\n",
    "    # Convert the frames to numpy arrays\n",
    "    depth_image = np.asanyarray(depth_frame.get_data())\n",
    "    color_image = np.asanyarray(color_frame.get_data())\n",
    "\n",
    "    # Perform object detection on the color image\n",
    "    results = model(color_image)\n",
    "\n",
    "    # Display the results\n",
    "    for result in results.xyxy[0]:\n",
    "        if result is not None:\n",
    "            xyxy = result[:4].tolist()\n",
    "            conf = result[4].item()\n",
    "            cls = int(result[5].item())\n",
    "\n",
    "            # Only draw bounding box and label if confidence is greater than 0.5\n",
    "            if conf > 0.5:\n",
    "                # Convert 2D bounding box to 3D\n",
    "                x_min, y_min, x_max, y_max = xyxy\n",
    "\n",
    "                # Create the object points for PnP\n",
    "                object_points = np.array([[0, 0, 0],\n",
    "                          [0, 1, 0],\n",
    "                          [1, 1, 0],\n",
    "                          [1, 0, 0]], dtype=np.float32)\n",
    "\n",
    "                # Create the image points for PnP\n",
    "                image_points = np.array([[x_min, y_min],\n",
    "                                         [x_min, y_max],\n",
    "                                         [x_max, y_max],\n",
    "                                         [x_max, y_min]])\n",
    "\n",
    "                # Perform PnP\n",
    "                success, rvec, tvec = cv2.solvePnP(object_points, image_points, camera_matrix, dist_coeffs)\n",
    "\n",
    "                if success:\n",
    "                    # Calculate tilt angle with respect to the camera\n",
    "                    _, _, yaw = cv2.Rodrigues(rvec)[0]\n",
    "                    rotation_matrix, _ = cv2.Rodrigues(rvec)\n",
    "                    tilt_vector = rotation_matrix[:, 1]  # Get the second column of the rotation matrix\n",
    "                    tilt_angle = math.degrees(math.atan2(tilt_vector[2], tilt_vector[1]))\n",
    "\n",
    "\n",
    "                    # Draw bounding box and label on the color image\n",
    "                    cv2.rectangle(color_image, (int(x_min), int(y_min)), (int(x_max), int(y_max)), (255, 0, 0), 2)\n",
    "                    cv2.putText(color_image, f'Tilt Angle: {tilt_angle:.2f} degrees', (int(x_min), int(y_min) - 10),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)\n",
    "\n",
    "    # Display the color image\n",
    "    cv2.imshow('Object Detection', color_image)\n",
    "\n",
    "    # Exit the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "# Stop the camera stream\n",
    "pipeline.stop()\n",
    "\n",
    "# Close all windows\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0993d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to C:\\Users\\Prabhu/.cache\\torch\\hub\\master.zip\n",
      "YOLOv5  2023-6-2 Python-3.10.4 torch-2.0.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7015519 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n",
      "YOLOv5  2023-6-2 Python-3.10.4 torch-2.0.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7015519 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import pyrealsense2 as rs\n",
    "\n",
    "# Load the YOLOv5 model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = torch.hub.load('ultralytics/yolov5', 'custom', 'best.pt', force_reload=True)\n",
    "model = torch.hub.load('yolov5', 'custom', 'best.pt', source='local')\n",
    "model.to(device).eval()\n",
    "\n",
    "# Set camera parameters\n",
    "width, height = 640, 480  # Set the desired frame size\n",
    "fps = 30  # Set the desired frame rate\n",
    "\n",
    "# Camera intrinsic parameters (example values)\n",
    "fx = 421.0  # Focal length in x-axis\n",
    "fy = 421.0  # Focal length in y-axis\n",
    "cx = 427.0  # Principal point x-coordinate\n",
    "cy = 236.0  # Principal point y-coordinate\n",
    "\n",
    "# Camera extrinsic parameters (example values)\n",
    "rotation_matrix = np.array([[   0.99997,  -0.007577,  -0.0027762],\n",
    "                            [  0.0075674,     0.99997,  -0.0034583],\n",
    "                            [  0.0028023,   0.0034372,     0.99999]])\n",
    "translation_vector = np.array([   0.015133,-4.4939e-05,0.0001245])\n",
    "\n",
    "# Initialize the RealSense camera\n",
    "pipeline = rs.pipeline()\n",
    "config = rs.config()\n",
    "config.enable_stream(rs.stream.color, width, height, rs.format.bgr8, fps)\n",
    "config.enable_stream(rs.stream.depth, width, height, rs.format.z16, fps)\n",
    "\n",
    "# Start the camera stream\n",
    "pipeline.start(config)\n",
    "\n",
    "# Object detection loop\n",
    "while True:\n",
    "    # Wait for a new frame\n",
    "    frames = pipeline.wait_for_frames()\n",
    "    color_frame = frames.get_color_frame()\n",
    "    depth_frame = frames.get_depth_frame()\n",
    "\n",
    "    # Convert the frames to numpy arrays\n",
    "    color_image = np.asanyarray(color_frame.get_data())\n",
    "    depth_image = np.asanyarray(depth_frame.get_data())\n",
    "\n",
    "    # Perform object detection\n",
    "    results = model(color_image)\n",
    "\n",
    "    # Display the results\n",
    "    for result in results.xyxy[0]:\n",
    "        if result is not None:\n",
    "            xyxy = result[:4].tolist()\n",
    "            conf = result[4].item()\n",
    "            cls = int(result[5].item())\n",
    "\n",
    "            # Only draw bounding box and label if confidence is greater than 0.7\n",
    "            if conf > 0.5:\n",
    "                # Draw bounding box and label on the frame\n",
    "                cv2.rectangle(color_image, (int(xyxy[0]), int(xyxy[1])), (int(xyxy[2]), int(xyxy[3])), (255, 0, 0), 2)\n",
    "                cv2.putText(color_image, f'{conf}', (int(xyxy[0]), int(xyxy[1]) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9,\n",
    "                            (255, 0, 0), 2)\n",
    "\n",
    "                # Get the top-left corner point of the bounding box\n",
    "                x, y = int(xyxy[0]), int(xyxy[1])\n",
    "\n",
    "                # Calculate the depth value at the top-left corner\n",
    "                depth = depth_image[y, x] / 1000.0  # Convert from millimeters to meters\n",
    "\n",
    "                # Calculate the camera coordinates (Xc, Yc, Zc)\n",
    "                Xc = (x - cx) * depth / fx\n",
    "                Yc = (y - cy) * depth / fy\n",
    "                Zc = depth\n",
    "\n",
    "                # Calculate the world coordinates (Xw, Yw, Zw) using camera extrinsics\n",
    "                world_coordinates = np.dot(rotation_matrix, np.array([Xc, Yc, Zc])) + translation_vector\n",
    "                Xw, Yw, Zw = world_coordinates\n",
    "\n",
    "                # Draw three perpendicular lines from the top-left corner\n",
    "                length = 50\n",
    "                cv2.line(color_image, (x, y), (x, y - length), (0, 0, 255), 2)  # Red line (vertical)\n",
    "                cv2.line(color_image, (x, y), (x + length, y), (0, 255, 0), 2)  # Green line (horizontal)\n",
    "                cv2.line(color_image, (x, y), (x + length, y - length), (255, 0, 0), 2)  # Blue line (diagonal)\n",
    "\n",
    "                # Display the world coordinates\n",
    "                cv2.putText(color_image, f'X: {Xw:.2f}', (x + 10, y + 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "                cv2.putText(color_image, f'Y: {Yw:.2f}', (x + 10, y + 30), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "                cv2.putText(color_image, f'Z: {Zw:.2f}', (x + 10, y + 50), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('Object Detection', color_image)\n",
    "\n",
    "    # Exit the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "# Stop the camera stream\n",
    "pipeline.stop()\n",
    "\n",
    "# Close all windows\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d7a2d4fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to C:\\Users\\Prabhu/.cache\\torch\\hub\\master.zip\n",
      "YOLOv5  2023-6-1 Python-3.10.4 torch-2.0.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7015519 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n",
      "YOLOv5  2023-6-1 Python-3.10.4 torch-2.0.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7015519 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "null pointer passed for argument \"frame_ref\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [33]\u001b[0m, in \u001b[0;36m<cell line: 29>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     33\u001b[0m depth_frame \u001b[38;5;241m=\u001b[39m frames\u001b[38;5;241m.\u001b[39mget_depth_frame()\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Convert the frames to numpy arrays\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m color_image \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masanyarray(\u001b[43mcolor_frame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     37\u001b[0m depth_image \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masanyarray(depth_frame\u001b[38;5;241m.\u001b[39mget_data())\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Perform object detection\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: null pointer passed for argument \"frame_ref\""
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import pyrealsense2 as rs\n",
    "\n",
    "# Load the YOLOv5 model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = torch.hub.load('ultralytics/yolov5', 'custom', 'best.pt', force_reload=True)\n",
    "model = torch.hub.load('yolov5', 'custom', 'best.pt', source='local')\n",
    "model.to(device).eval()\n",
    "\n",
    "# Initialize the RealSense camera\n",
    "pipeline = rs.pipeline()\n",
    "config = rs.config()\n",
    "\n",
    "# Enable depth stream and retrieve camera calibration\n",
    "config.enable_stream(rs.stream.depth, 0, 0, rs.format.z16, 30)\n",
    "pipeline.start(config)\n",
    "profile = pipeline.get_active_profile()\n",
    "depth_sensor = profile.get_device().first_depth_sensor()\n",
    "depth_scale = depth_sensor.get_depth_scale()\n",
    "intrinsics = profile.get_stream(rs.stream.depth).as_video_stream_profile().get_intrinsics()\n",
    "\n",
    "# Set camera parameters\n",
    "width, height = intrinsics.width, intrinsics.height\n",
    "fx, fy, cx, cy = intrinsics.fx, intrinsics.fy, intrinsics.ppx, intrinsics.ppy\n",
    "\n",
    "# Object detection loop\n",
    "while True:\n",
    "    # Wait for a new frame\n",
    "    frames = pipeline.wait_for_frames()\n",
    "    color_frame = frames.get_color_frame()\n",
    "    depth_frame = frames.get_depth_frame()\n",
    "\n",
    "    # Convert the frames to numpy arrays\n",
    "    color_image = np.asanyarray(color_frame.get_data())\n",
    "    depth_image = np.asanyarray(depth_frame.get_data())\n",
    "\n",
    "    # Perform object detection\n",
    "    results = model(color_image)\n",
    "\n",
    "    # Display the results\n",
    "    for result in results.xyxy[0]:\n",
    "        if result is not None:\n",
    "            xyxy = result[:4].tolist()\n",
    "            conf = result[4].item()\n",
    "            cls = int(result[5].item())\n",
    "\n",
    "            # Only draw bounding box and label if confidence is greater than 0.5\n",
    "            if conf > 0.5:\n",
    "                # Draw bounding box and label on the frame\n",
    "                cv2.rectangle(color_image, (int(xy, 0), 2))\n",
    "                cv2.putText(color_image, f'{conf}', (int(xyxy[0]), int(xyxy[1]) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9,\n",
    "                            (255, 0, 0), 2)\n",
    "\n",
    "                # Get the top-left corner point of the bounding box\n",
    "                x, y = int(xyxy[0]), int(xyxy[1])\n",
    "\n",
    "                # Calculate the depth value at the top-left corner\n",
    "                depth = depth_image[y, x] / depth_scale  # Convert to meters\n",
    "\n",
    "                # Calculate the camera coordinates (Xc, Yc, Zc)\n",
    "                Xc = (x - cx) * depth / fx\n",
    "                Yc = (y - cy) * depth / fy\n",
    "                Zc = depth\n",
    "\n",
    "                # Draw three perpendicular lines from the top-left corner\n",
    "                length = 50\n",
    "                cv2.line(color_image, (x, y), (x, y - length), (0, 0, 255), 2)  # Red line (vertical)\n",
    "                cv2.line(color_image, (x, y), (x + length, y), (0, 255, 0), 2)  # Green line (horizontal)\n",
    "                cv2.line(color_image, (x, y), (x + length, y - length), (255, 0, 0), 2)  # Blue line (diagonal)\n",
    "\n",
    "                # Display the camera coordinates\n",
    "                cv2.putText(color_image, f'Xc: {Xc:.2f}', (x + 10, y + 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n",
    "                            (255, 255, 255), 2)\n",
    "                cv2.putText(color_image, f'Yc: {Yc:.2f}', (x + 10, y + 30), cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n",
    "                            (255, 255, 255), 2)\n",
    "                cv2.putText(color_image, f'Zc: {Zc:.2f}', (x + 10, y + 50), cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n",
    "                            (255, 255, 255), 2)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('Object Detection', color_image)\n",
    "\n",
    "    # Exit the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "# Stop the camera stream\n",
    "pipeline.stop()\n",
    "\n",
    "# Close all windows\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "06af0db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intrinsic Parameters:\n",
      "- fx: 421.83587646484375\n",
      "- fy: 421.83587646484375\n",
      "- cx: 427.6553039550781\n",
      "- cy: 236.85336303710938\n",
      "\n",
      "Rotation Matrix:\n",
      "[[    0.99997   -0.007577  -0.0027762]\n",
      " [  0.0075674     0.99997  -0.0034583]\n",
      " [  0.0028023   0.0034372     0.99999]]\n",
      "\n",
      "Translation Vector:\n",
      "[   0.015133 -4.4939e-05   0.0001245]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7610d940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intrinsic Parameters:\n",
      "- fx: 421.83587646484375\n",
      "- fy: 421.83587646484375\n",
      "- cx: 427.6553039550781\n",
      "- cy: 236.85336303710938\n",
      "\n",
      "Rotation Matrix:\n",
      "[[    0.99997   -0.007577  -0.0027762]\n",
      " [  0.0075674     0.99997  -0.0034583]\n",
      " [  0.0028023   0.0034372     0.99999]]\n",
      "\n",
      "Translation Vector:\n",
      "[   0.015133 -4.4939e-05   0.0001245]\n"
     ]
    }
   ],
   "source": [
    "import pyrealsense2 as rs\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the RealSense camera\n",
    "pipeline = rs.pipeline()\n",
    "config = rs.config()\n",
    "config.enable_stream(rs.stream.depth, 0, 0, rs.format.z16, 30)\n",
    "config.enable_stream(rs.stream.color, 0, 0, rs.format.bgr8, 30)\n",
    "pipeline.start(config)\n",
    "\n",
    "# Wait for a frameset\n",
    "frameset = pipeline.wait_for_frames()\n",
    "depth_frame = frameset.get_depth_frame()\n",
    "color_frame = frameset.get_color_frame()\n",
    "\n",
    "# Obtain the camera calibration intrinsics\n",
    "depth_intrinsics = depth_frame.profile.as_video_stream_profile().intrinsics\n",
    "color_intrinsics = color_frame.profile.as_video_stream_profile().intrinsics\n",
    "\n",
    "# Get the intrinsic parameters\n",
    "fx = depth_intrinsics.fx\n",
    "fy = depth_intrinsics.fy\n",
    "cx = depth_intrinsics.ppx\n",
    "cy = depth_intrinsics.ppy\n",
    "\n",
    "# Obtain the camera extrinsics\n",
    "extrinsics = depth_frame.profile.get_extrinsics_to(color_frame.profile)\n",
    "\n",
    "# Get the rotation matrix\n",
    "rotation_matrix = np.reshape(extrinsics.rotation, (3, 3))\n",
    "\n",
    "# Get the translation vector\n",
    "translation_vector = np.array([extrinsics.translation[0], extrinsics.translation[1], extrinsics.translation[2]])\n",
    "\n",
    "# Print the intrinsic parameters, rotation matrix, and translation vector\n",
    "print(\"Intrinsic Parameters:\")\n",
    "print(f\"- fx: {fx}\")\n",
    "print(f\"- fy: {fy}\")\n",
    "print(f\"- cx: {cx}\")\n",
    "print(f\"- cy: {cy}\")\n",
    "\n",
    "print(\"\\nRotation Matrix:\")\n",
    "print(rotation_matrix)\n",
    "\n",
    "print(\"\\nTranslation Vector:\")\n",
    "print(translation_vector)\n",
    "\n",
    "# Stop the camera stream\n",
    "pipeline.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febe4c76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
