{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "421b406e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to C:\\Users\\Prabhu/.cache\\torch\\hub\\master.zip\n",
      "YOLOv5  2023-6-1 Python-3.10.4 torch-2.0.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7015519 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.7.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\calib3d\\src\\solvepnp.cpp:840: error: (-215:Assertion failed) ( (npoints >= 4) || (npoints == 3 && flags == SOLVEPNP_ITERATIVE && useExtrinsicGuess) || (npoints >= 3 && flags == SOLVEPNP_SQPNP) ) && npoints == std::max(ipoints.checkVector(2, CV_32F), ipoints.checkVector(2, CV_64F)) in function 'cv::solvePnPGeneric'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 44>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     86\u001b[0m image_points \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([[x_min, y_min],\n\u001b[0;32m     87\u001b[0m                          [x_min, y_max],\n\u001b[0;32m     88\u001b[0m                          [x_max, y_max],\n\u001b[0;32m     89\u001b[0m                          [x_max, y_min],\n\u001b[0;32m     90\u001b[0m                          [\u001b[38;5;28mint\u001b[39m((x_min \u001b[38;5;241m+\u001b[39m x_max) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m), \u001b[38;5;28mint\u001b[39m((y_min \u001b[38;5;241m+\u001b[39m y_max) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m)]])\n\u001b[0;32m     92\u001b[0m \u001b[38;5;66;03m# Perform PnP\u001b[39;00m\n\u001b[1;32m---> 93\u001b[0m success, rvec, tvec \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msolvePnP\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobject_points\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_points\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcamera_matrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdist_coeffs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;66;03m# Calculate tilt angle with respect to the camera\u001b[39;00m\n\u001b[0;32m     97\u001b[0m     _, _, yaw \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mRodrigues(rvec)[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.7.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\calib3d\\src\\solvepnp.cpp:840: error: (-215:Assertion failed) ( (npoints >= 4) || (npoints == 3 && flags == SOLVEPNP_ITERATIVE && useExtrinsicGuess) || (npoints >= 3 && flags == SOLVEPNP_SQPNP) ) && npoints == std::max(ipoints.checkVector(2, CV_32F), ipoints.checkVector(2, CV_64F)) in function 'cv::solvePnPGeneric'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import pyrealsense2 as rs\n",
    "import math\n",
    "\n",
    "# Load the YOLOv5 model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = torch.hub.load('ultralytics/yolov5', 'custom', 'best.pt', force_reload=True)\n",
    "model.to(device).eval()\n",
    "\n",
    "# Set camera parameters\n",
    "width, height = 640, 480  # Set the desired frame size\n",
    "fps = 30  # Set the desired frame rate\n",
    "\n",
    "# Initialize the RealSense camera\n",
    "pipeline = rs.pipeline()\n",
    "config = rs.config()\n",
    "config.enable_stream(rs.stream.depth, width, height, rs.format.z16, fps)\n",
    "config.enable_stream(rs.stream.color, width, height, rs.format.bgr8, fps)\n",
    "\n",
    "# Start the camera stream\n",
    "pipeline.start(config)\n",
    "\n",
    "# Create a pipeline profile to get the camera intrinsics\n",
    "profile = pipeline.get_active_profile()\n",
    "depth_profile = rs.video_stream_profile(profile.get_stream(rs.stream.depth))\n",
    "depth_intrinsics = depth_profile.get_intrinsics()\n",
    "\n",
    "# Retrieve camera calibration parameters\n",
    "fx = depth_intrinsics.fx\n",
    "fy = depth_intrinsics.fy\n",
    "cx = depth_intrinsics.ppx\n",
    "cy = depth_intrinsics.ppy\n",
    "k1, k2, p1, p2, k3 = depth_intrinsics.coeffs\n",
    "\n",
    "# Load camera matrix and distortion coefficients\n",
    "camera_matrix = np.array([[fx, 0, cx],\n",
    "                          [0, fy, cy],\n",
    "                          [0, 0, 1]])\n",
    "dist_coeffs = np.array([k1, k2, p1, p2, k3])\n",
    "\n",
    "# Object detection loop\n",
    "while True:\n",
    "    # Wait for a new frame\n",
    "    frames = pipeline.wait_for_frames()\n",
    "    depth_frame = frames.get_depth_frame()\n",
    "    color_frame = frames.get_color_frame()\n",
    "\n",
    "    # Convert the frames to numpy arrays\n",
    "    depth_image = np.asanyarray(depth_frame.get_data())\n",
    "    color_image = np.asanyarray(color_frame.get_data())\n",
    "\n",
    "    # Perform object detection on the color image\n",
    "    results = model(color_image)\n",
    "\n",
    "    # Display the results\n",
    "    for result in results.xyxy[0]:\n",
    "        if result is not None:\n",
    "            xyxy = result[:4].tolist()\n",
    "            conf = result[4].item()\n",
    "            cls = int(result[5].item())\n",
    "\n",
    "            # Only draw bounding box and label if confidence is greater than 0.5\n",
    "            if conf > 0.5:\n",
    "                # Convert 2D bounding box to 3D\n",
    "                x_min, y_min, x_max, y_max = xyxy\n",
    "\n",
    "                # Find the depth value at the center of the bounding box\n",
    "                x_center = int((x_min + x_max) / 2)\n",
    "                y_center = int((y_min + y_max) / 2)\n",
    "                depth_value = depth_frame.get_distance(x_center, y_center)\n",
    "\n",
    "                # Convert the depth value to 3D coordinates\n",
    "                depth_point = rs.rs2_deproject_pixel_to_point(depth_intrinsics, [x_center, y_center], depth_value)\n",
    "                x, y, z = depth_point\n",
    "\n",
    "                # Create the object points for PnP\n",
    "                object_points = np.array([[0, 0, 0],\n",
    "                                          [0, 1, 0],\n",
    "                                          [1, 1, 0],\n",
    "                                          [1, 0, 0],\n",
    "                                          [1, 1, 0]])\n",
    "\n",
    "                # Create the image points for PnP\n",
    "                image_points = np.array([[x_min, y_min],\n",
    "                                         [x_min, y_max],\n",
    "                                         [x_max, y_max],\n",
    "                                         [x_max, y_min],\n",
    "                                         [int((x_min + x_max) / 2), int((y_min + y_max) / 2)]])\n",
    "\n",
    "                # Perform PnP\n",
    "                success, rvec, tvec = cv2.solvePnP(object_points, image_points, camera_matrix, dist_coeffs)\n",
    "\n",
    "                if success:\n",
    "                    # Calculate tilt angle with respect to the camera\n",
    "                    _, _, yaw = cv2.Rodrigues(rvec)[0]\n",
    "                    tilt_angle = math.degrees(yaw)\n",
    "\n",
    "                    # Draw bounding box and label on the color image\n",
    "                    cv2.rectangle(color_image, (int(x_min), int(y_min)), (int(x_max), int(y_max)), (255, 0, 0), 2)\n",
    "                    cv2.putText(color_image, f'Tilt Angle: {tilt_angle:.2f} degrees', (int(x_min), int(y_min) - 10),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)\n",
    "\n",
    "    # Display the color image\n",
    "    cv2.imshow('Object Detection', color_image)\n",
    "\n",
    "    # Exit the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "# Stop the camera stream\n",
    "pipeline.stop()\n",
    "\n",
    "# Close all windows\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "175418f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to C:\\Users\\Prabhu/.cache\\torch\\hub\\master.zip\n",
      "YOLOv5  2023-6-2 Python-3.10.4 torch-2.0.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7015519 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n",
      "YOLOv5  2023-6-2 Python-3.10.4 torch-2.0.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7015519 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import pyrealsense2 as rs\n",
    "\n",
    "# Load the YOLOv5 model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = torch.hub.load('ultralytics/yolov5', 'custom', 'best.pt',force_reload=True)\n",
    "model = torch.hub.load('yolov5', 'custom', 'best.pt',source='local')\n",
    "model.to(device).eval()\n",
    "\n",
    "# Set camera parametersq\n",
    "width, height = 640, 480  # Set the desired frame size\n",
    "fps = 30  # Set the desired frame rate\n",
    "\n",
    "# Initialize the RealSense camera\n",
    "pipeline = rs.pipeline()\n",
    "config = rs.config()\n",
    "config.enable_stream(rs.stream.color, width, height, rs.format.bgr8, fps)\n",
    "\n",
    "# Start the camera stream\n",
    "pipeline.start(config)\n",
    "\n",
    "# Object detection loop\n",
    "while True:\n",
    "    # Wait for a new frame\n",
    "    frames = pipeline.wait_for_frames()\n",
    "    color_frame = frames.get_color_frame()\n",
    "    \n",
    "    # Convert the frame to a numpy array\n",
    "    frame = np.asanyarray(color_frame.get_data())\n",
    "\n",
    "    # Perform object detection\n",
    "    results = model(frame)\n",
    "\n",
    "    # Display the results\n",
    "    for result in results.xyxy[0]:\n",
    "        if result is not None:\n",
    "            xyxy = result[:4].tolist()\n",
    "            conf = result[4].item()\n",
    "            cls = int(result[5].item())\n",
    "\n",
    "            # Only draw bounding box and label if confidence is greater than 0.7\n",
    "            if conf > 0.5:\n",
    "                # Draw bounding box and label on the frame\n",
    "                cv2.rectangle(frame, (int(xyxy[0]), int(xyxy[1])), (int(xyxy[2]), int(xyxy[3])), (255, 0, 0), 2)\n",
    "                cv2.putText(frame, f'{cls}', (int(xyxy[0]), int(xyxy[1]) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('Object Detection', frame)\n",
    "\n",
    "    # Exit the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "# Stop the camera stream\n",
    "pipeline.stop()\n",
    "\n",
    "# Close all windows\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4a95b97b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to C:\\Users\\Prabhu/.cache\\torch\\hub\\master.zip\n",
      "YOLOv5  2023-6-1 Python-3.10.4 torch-2.0.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7015519 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import pyrealsense2 as rs\n",
    "import math\n",
    "\n",
    "# Load the YOLOv5 model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = torch.hub.load('ultralytics/yolov5', 'custom', 'best.pt', force_reload=True)\n",
    "model.to(device).eval()\n",
    "\n",
    "# Set camera parameters\n",
    "width, height = 640, 480  # Set the desired frame size\n",
    "fps = 30  # Set the desired frame rate\n",
    "\n",
    "# Initialize the RealSense camera\n",
    "pipeline = rs.pipeline()\n",
    "config = rs.config()\n",
    "config.enable_stream(rs.stream.depth, width, height, rs.format.z16, fps)\n",
    "config.enable_stream(rs.stream.color, width, height, rs.format.bgr8, fps)\n",
    "\n",
    "# Start the camera stream\n",
    "pipeline.start(config)\n",
    "\n",
    "# Create a pipeline profile to get the camera intrinsics\n",
    "profile = pipeline.get_active_profile()\n",
    "depth_profile = rs.video_stream_profile(profile.get_stream(rs.stream.depth))\n",
    "depth_intrinsics = depth_profile.get_intrinsics()\n",
    "\n",
    "# Retrieve camera calibration parameters\n",
    "fx = depth_intrinsics.fx\n",
    "fy = depth_intrinsics.fy\n",
    "cx = depth_intrinsics.ppx\n",
    "cy = depth_intrinsics.ppy\n",
    "k1, k2, p1, p2, k3 = depth_intrinsics.coeffs\n",
    "\n",
    "# Load camera matrix and distortion coefficients\n",
    "camera_matrix = np.array([[fx, 0, cx],\n",
    "                          [0, fy, cy],\n",
    "                          [0, 0, 1]])\n",
    "dist_coeffs = np.array([k1, k2, p1, p2, k3])\n",
    "\n",
    "# Object detection loop\n",
    "while True:\n",
    "    # Wait for a new frame\n",
    "    frames = pipeline.wait_for_frames()\n",
    "    depth_frame = frames.get_depth_frame()\n",
    "    color_frame = frames.get_color_frame()\n",
    "\n",
    "    # Convert the frames to numpy arrays\n",
    "    depth_image = np.asanyarray(depth_frame.get_data())\n",
    "    color_image = np.asanyarray(color_frame.get_data())\n",
    "\n",
    "    # Perform object detection on the color image\n",
    "    results = model(color_image)\n",
    "\n",
    "    # Display the results\n",
    "    for result in results.xyxy[0]:\n",
    "        if result is not None:\n",
    "            xyxy = result[:4].tolist()\n",
    "            conf = result[4].item()\n",
    "            cls = int(result[5].item())\n",
    "\n",
    "            # Only draw bounding box and label if confidence is greater than 0.5\n",
    "            if conf > 0.5:\n",
    "                # Convert 2D bounding box to 3D\n",
    "                x_min, y_min, x_max, y_max = xyxy\n",
    "\n",
    "                # Create the object points for PnP\n",
    "                object_points = np.array([[0, 0, 0],\n",
    "                          [0, 1, 0],\n",
    "                          [1, 1, 0],\n",
    "                          [1, 0, 0]], dtype=np.float32)\n",
    "\n",
    "                # Create the image points for PnP\n",
    "                image_points = np.array([[x_min, y_min],\n",
    "                                         [x_min, y_max],\n",
    "                                         [x_max, y_max],\n",
    "                                         [x_max, y_min]])\n",
    "\n",
    "                # Perform PnP\n",
    "                success, rvec, tvec = cv2.solvePnP(object_points, image_points, camera_matrix, dist_coeffs)\n",
    "\n",
    "                if success:\n",
    "                    # Calculate tilt angle with respect to the camera\n",
    "                    _, _, yaw = cv2.Rodrigues(rvec)[0]\n",
    "                    rotation_matrix, _ = cv2.Rodrigues(rvec)\n",
    "                    tilt_vector = rotation_matrix[:, 1]  # Get the second column of the rotation matrix\n",
    "                    tilt_angle = math.degrees(math.atan2(tilt_vector[2], tilt_vector[1]))\n",
    "\n",
    "\n",
    "                    # Draw bounding box and label on the color image\n",
    "                    cv2.rectangle(color_image, (int(x_min), int(y_min)), (int(x_max), int(y_max)), (255, 0, 0), 2)\n",
    "                    cv2.putText(color_image, f'Tilt Angle: {tilt_angle:.2f} degrees', (int(x_min), int(y_min) - 10),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)\n",
    "\n",
    "    # Display the color image\n",
    "    cv2.imshow('Object Detection', color_image)\n",
    "\n",
    "    # Exit the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "# Stop the camera stream\n",
    "pipeline.stop()\n",
    "\n",
    "# Close all windows\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0993d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to C:\\Users\\Prabhu/.cache\\torch\\hub\\master.zip\n",
      "YOLOv5  2023-6-7 Python-3.10.4 torch-2.0.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7015519 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n",
      "YOLOv5  2023-6-7 Python-3.10.4 torch-2.0.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7015519 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import pyrealsense2 as rs\n",
    "\n",
    "# Load the YOLOv5 model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = torch.hub.load('ultralytics/yolov5', 'custom', 'best.pt', force_reload=True)\n",
    "model = torch.hub.load('yolov5', 'custom', 'best.pt', source='local')\n",
    "model.to(device).eval()\n",
    "\n",
    "# Set camera parameters\n",
    "width, height = 640, 480  # Set the desired frame size\n",
    "fps = 30  # Set the desired frame rate\n",
    "\n",
    "# Camera intrinsic parameters (example values)\n",
    "fx = 421.0  # Focal length in x-axis\n",
    "fy = 421.0  # Focal length in y-axis\n",
    "cx = 427.0  # Principal point x-coordinate\n",
    "cy = 236.0  # Principal point y-coordinate\n",
    "\n",
    "# Camera extrinsic parameters (example values)\n",
    "rotation_matrix = np.array([[   0.99997,  -0.007577,  -0.0027762],\n",
    "                            [  0.0075674,     0.99997,  -0.0034583],\n",
    "                            [  0.0028023,   0.0034372,     0.99999]])\n",
    "translation_vector = np.array([   0.015133,-4.4939e-05,0.0001245])\n",
    "\n",
    "# Initialize the RealSense camera\n",
    "pipeline = rs.pipeline()\n",
    "config = rs.config()\n",
    "config.enable_stream(rs.stream.color, width, height, rs.format.bgr8, fps)\n",
    "config.enable_stream(rs.stream.depth, width, height, rs.format.z16, fps)\n",
    "\n",
    "# Start the camera stream\n",
    "pipeline.start(config)\n",
    "\n",
    "# Object detection loop\n",
    "while True:\n",
    "    # Wait for a new frame\n",
    "    frames = pipeline.wait_for_frames()\n",
    "    color_frame = frames.get_color_frame()\n",
    "    depth_frame = frames.get_depth_frame()\n",
    "\n",
    "    # Convert the frames to numpy arrays\n",
    "    color_image = np.asanyarray(color_frame.get_data())\n",
    "    depth_image = np.asanyarray(depth_frame.get_data())\n",
    "\n",
    "    # Perform object detection\n",
    "    results = model(color_image)\n",
    "\n",
    "    # Display the results\n",
    "    for result in results.xyxy[0]:\n",
    "        if result is not None:\n",
    "            xyxy = result[:4].tolist()\n",
    "            conf = result[4].item()\n",
    "            cls = int(result[5].item())\n",
    "\n",
    "            # Only draw bounding box and label if confidence is greater than 0.7\n",
    "            if conf > 0.5:\n",
    "                # Draw bounding box and label on the frame\n",
    "                cv2.rectangle(color_image, (int(xyxy[0]), int(xyxy[1])), (int(xyxy[2]), int(xyxy[3])), (255, 0, 0), 2)\n",
    "                cv2.putText(color_image, f'{conf}', (int(xyxy[0]), int(xyxy[1]) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9,\n",
    "                            (255, 0, 0), 2)\n",
    "\n",
    "                # Get the top-left corner point of the bounding box\n",
    "                x, y = int(xyxy[0]), int(xyxy[1])\n",
    "\n",
    "                # Calculate the depth value at the top-left corner\n",
    "                depth = depth_image[y, x] / 1000.0  # Convert from millimeters to meters\n",
    "\n",
    "                # Calculate the camera coordinates (Xc, Yc, Zc)\n",
    "                Xc = (x - cx) * depth / fx\n",
    "                Yc = (y - cy) * depth / fy\n",
    "                Zc = depth\n",
    "\n",
    "                # Calculate the world coordinates (Xw, Yw, Zw) using camera extrinsics\n",
    "                world_coordinates = np.dot(rotation_matrix, np.array([Xc, Yc, Zc])) + translation_vector\n",
    "                Xw, Yw, Zw = world_coordinates\n",
    "\n",
    "                # Draw three perpendicular lines from the top-left corner\n",
    "                length = 50\n",
    "                cv2.line(color_image, (x, y), (x, y - length), (0, 0, 255), 2)  # Red line (vertical)\n",
    "                cv2.line(color_image, (x, y), (x + length, y), (0, 255, 0), 2)  # Green line (horizontal)\n",
    "                cv2.line(color_image, (x, y), (x + length, y - length), (255, 0, 0), 2)  # Blue line (diagonal)\n",
    "\n",
    "                # Display the world coordinates\n",
    "                cv2.putText(color_image, f'X: {Xw:.2f}', (x + 10, y + 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "                cv2.putText(color_image, f'Y: {Yw:.2f}', (x + 10, y + 30), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "                cv2.putText(color_image, f'Z: {Zw:.2f}', (x + 10, y + 50), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('Object Detection', color_image)\n",
    "\n",
    "    # Exit the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "# Stop the camera stream\n",
    "pipeline.stop()\n",
    "\n",
    "# Close all windows\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d7a2d4fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to C:\\Users\\Prabhu/.cache\\torch\\hub\\master.zip\n",
      "YOLOv5  2023-6-1 Python-3.10.4 torch-2.0.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7015519 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n",
      "YOLOv5  2023-6-1 Python-3.10.4 torch-2.0.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7015519 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "null pointer passed for argument \"frame_ref\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [33]\u001b[0m, in \u001b[0;36m<cell line: 29>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     33\u001b[0m depth_frame \u001b[38;5;241m=\u001b[39m frames\u001b[38;5;241m.\u001b[39mget_depth_frame()\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Convert the frames to numpy arrays\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m color_image \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masanyarray(\u001b[43mcolor_frame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     37\u001b[0m depth_image \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masanyarray(depth_frame\u001b[38;5;241m.\u001b[39mget_data())\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Perform object detection\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: null pointer passed for argument \"frame_ref\""
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import pyrealsense2 as rs\n",
    "\n",
    "# Load the YOLOv5 model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = torch.hub.load('ultralytics/yolov5', 'custom', 'best.pt', force_reload=True)\n",
    "model = torch.hub.load('yolov5', 'custom', 'best.pt', source='local')\n",
    "model.to(device).eval()\n",
    "\n",
    "# Initialize the RealSense camera\n",
    "pipeline = rs.pipeline()\n",
    "config = rs.config()\n",
    "\n",
    "# Enable depth stream and retrieve camera calibration\n",
    "config.enable_stream(rs.stream.depth, 0, 0, rs.format.z16, 30)\n",
    "pipeline.start(config)\n",
    "profile = pipeline.get_active_profile()\n",
    "depth_sensor = profile.get_device().first_depth_sensor()\n",
    "depth_scale = depth_sensor.get_depth_scale()\n",
    "intrinsics = profile.get_stream(rs.stream.depth).as_video_stream_profile().get_intrinsics()\n",
    "\n",
    "# Set camera parameters\n",
    "width, height = intrinsics.width, intrinsics.height\n",
    "fx, fy, cx, cy = intrinsics.fx, intrinsics.fy, intrinsics.ppx, intrinsics.ppy\n",
    "\n",
    "# Object detection loop\n",
    "while True:\n",
    "    # Wait for a new frame\n",
    "    frames = pipeline.wait_for_frames()\n",
    "    color_frame = frames.get_color_frame()\n",
    "    depth_frame = frames.get_depth_frame()\n",
    "\n",
    "    # Convert the frames to numpy arrays\n",
    "    color_image = np.asanyarray(color_frame.get_data())\n",
    "    depth_image = np.asanyarray(depth_frame.get_data())\n",
    "\n",
    "    # Perform object detection\n",
    "    results = model(color_image)\n",
    "\n",
    "    # Display the results\n",
    "    for result in results.xyxy[0]:\n",
    "        if result is not None:\n",
    "            xyxy = result[:4].tolist()\n",
    "            conf = result[4].item()\n",
    "            cls = int(result[5].item())\n",
    "\n",
    "            # Only draw bounding box and label if confidence is greater than 0.5\n",
    "            if conf > 0.5:\n",
    "                # Draw bounding box and label on the frame\n",
    "                cv2.rectangle(color_image, (int(xy, 0), 2))\n",
    "                cv2.putText(color_image, f'{conf}', (int(xyxy[0]), int(xyxy[1]) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9,\n",
    "                            (255, 0, 0), 2)\n",
    "\n",
    "                # Get the top-left corner point of the bounding box\n",
    "                x, y = int(xyxy[0]), int(xyxy[1])\n",
    "\n",
    "                # Calculate the depth value at the top-left corner\n",
    "                depth = depth_image[y, x] / depth_scale  # Convert to meters\n",
    "\n",
    "                # Calculate the camera coordinates (Xc, Yc, Zc)\n",
    "                Xc = (x - cx) * depth / fx\n",
    "                Yc = (y - cy) * depth / fy\n",
    "                Zc = depth\n",
    "\n",
    "                # Draw three perpendicular lines from the top-left corner\n",
    "                length = 50\n",
    "                cv2.line(color_image, (x, y), (x, y - length), (0, 0, 255), 2)  # Red line (vertical)\n",
    "                cv2.line(color_image, (x, y), (x + length, y), (0, 255, 0), 2)  # Green line (horizontal)\n",
    "                cv2.line(color_image, (x, y), (x + length, y - length), (255, 0, 0), 2)  # Blue line (diagonal)\n",
    "\n",
    "                # Display the camera coordinates\n",
    "                cv2.putText(color_image, f'Xc: {Xc:.2f}', (x + 10, y + 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n",
    "                            (255, 255, 255), 2)\n",
    "                cv2.putText(color_image, f'Yc: {Yc:.2f}', (x + 10, y + 30), cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n",
    "                            (255, 255, 255), 2)\n",
    "                cv2.putText(color_image, f'Zc: {Zc:.2f}', (x + 10, y + 50), cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n",
    "                            (255, 255, 255), 2)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('Object Detection', color_image)\n",
    "\n",
    "    # Exit the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "# Stop the camera stream\n",
    "pipeline.stop()\n",
    "\n",
    "# Close all windows\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "06af0db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intrinsic Parameters:\n",
      "- fx: 421.83587646484375\n",
      "- fy: 421.83587646484375\n",
      "- cx: 427.6553039550781\n",
      "- cy: 236.85336303710938\n",
      "\n",
      "Rotation Matrix:\n",
      "[[    0.99997   -0.007577  -0.0027762]\n",
      " [  0.0075674     0.99997  -0.0034583]\n",
      " [  0.0028023   0.0034372     0.99999]]\n",
      "\n",
      "Translation Vector:\n",
      "[   0.015133 -4.4939e-05   0.0001245]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7610d940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intrinsic Parameters:\n",
      "- fx: 421.83587646484375\n",
      "- fy: 421.83587646484375\n",
      "- cx: 427.6553039550781\n",
      "- cy: 236.85336303710938\n",
      "\n",
      "Rotation Matrix:\n",
      "[[    0.99997   -0.007577  -0.0027762]\n",
      " [  0.0075674     0.99997  -0.0034583]\n",
      " [  0.0028023   0.0034372     0.99999]]\n",
      "\n",
      "Translation Vector:\n",
      "[   0.015133 -4.4939e-05   0.0001245]\n"
     ]
    }
   ],
   "source": [
    "import pyrealsense2 as rs\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the RealSense camera\n",
    "pipeline = rs.pipeline()\n",
    "config = rs.config()\n",
    "config.enable_stream(rs.stream.depth, 0, 0, rs.format.z16, 30)\n",
    "config.enable_stream(rs.stream.color, 0, 0, rs.format.bgr8, 30)\n",
    "pipeline.start(config)\n",
    "\n",
    "# Wait for a frameset\n",
    "frameset = pipeline.wait_for_frames()\n",
    "depth_frame = frameset.get_depth_frame()\n",
    "color_frame = frameset.get_color_frame()\n",
    "\n",
    "# Obtain the camera calibration intrinsics\n",
    "depth_intrinsics = depth_frame.profile.as_video_stream_profile().intrinsics\n",
    "color_intrinsics = color_frame.profile.as_video_stream_profile().intrinsics\n",
    "\n",
    "# Get the intrinsic parameters\n",
    "fx = depth_intrinsics.fx\n",
    "fy = depth_intrinsics.fy\n",
    "cx = depth_intrinsics.ppx\n",
    "cy = depth_intrinsics.ppy\n",
    "\n",
    "# Obtain the camera extrinsics\n",
    "extrinsics = depth_frame.profile.get_extrinsics_to(color_frame.profile)\n",
    "\n",
    "# Get the rotation matrix\n",
    "rotation_matrix = np.reshape(extrinsics.rotation, (3, 3))\n",
    "\n",
    "# Get the translation vector\n",
    "translation_vector = np.array([extrinsics.translation[0], extrinsics.translation[1], extrinsics.translation[2]])\n",
    "\n",
    "# Print the intrinsic parameters, rotation matrix, and translation vector\n",
    "print(\"Intrinsic Parameters:\")\n",
    "print(f\"- fx: {fx}\")\n",
    "print(f\"- fy: {fy}\")\n",
    "print(f\"- cx: {cx}\")\n",
    "print(f\"- cy: {cy}\")\n",
    "\n",
    "print(\"\\nRotation Matrix:\")\n",
    "print(rotation_matrix)\n",
    "\n",
    "print(\"\\nTranslation Vector:\")\n",
    "print(translation_vector)\n",
    "\n",
    "# Stop the camera stream\n",
    "pipeline.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "febe4c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to C:\\Users\\Prabhu/.cache\\torch\\hub\\master.zip\n",
      "YOLOv5  2023-6-7 Python-3.10.4 torch-2.0.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7015519 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n",
      "YOLOv5  2023-6-7 Python-3.10.4 torch-2.0.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7015519 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import pyrealsense2 as rs\n",
    "\n",
    "# Load the YOLOv5 model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = torch.hub.load('ultralytics/yolov5', 'custom', 'best.pt', force_reload=True)\n",
    "model = torch.hub.load('yolov5', 'custom', 'best.pt', source='local')\n",
    "model.to(device).eval()\n",
    "\n",
    "# Set camera parameters\n",
    "width, height = 640, 480  # Set the desired frame size\n",
    "fps = 30  # Set the desired frame rate\n",
    "\n",
    "# Initialize the RealSense camera\n",
    "pipeline = rs.pipeline()\n",
    "config = rs.config()\n",
    "config.enable_stream(rs.stream.color, width, height, rs.format.bgr8, fps)\n",
    "config.enable_stream(rs.stream.depth, width, height, rs.format.z16, fps)\n",
    "\n",
    "# Start the camera stream\n",
    "pipeline.start(config)\n",
    "\n",
    "# Object detection loop\n",
    "while True:\n",
    "    # Wait for a new frame\n",
    "    frames = pipeline.wait_for_frames()\n",
    "    color_frame = frames.get_color_frame()\n",
    "    depth_frame = frames.get_depth_frame()\n",
    "\n",
    "    # Convert the frames to numpy arrays\n",
    "    color_image = np.asanyarray(color_frame.get_data())\n",
    "    depth_image = np.asanyarray(depth_frame.get_data()).astype(np.float32)\n",
    "\n",
    "    # Normalize the depth values\n",
    "    depth_image /= 1000.0  # Convert from millimeters to meters\n",
    "\n",
    "    # Perform object detection\n",
    "    results = model(depth_image)\n",
    "\n",
    "    # Display the results in the color image\n",
    "    for result in results.xyxy[0]:\n",
    "        if result is not None:\n",
    "            xyxy = result[:4].tolist()\n",
    "            conf = result[4].item()\n",
    "            cls = int(result[5].item())\n",
    "\n",
    "            # Only draw bounding box and label if confidence is greater than 0.5\n",
    "            if conf > 0.5:\n",
    "                # Draw bounding box and label on the color image\n",
    "                cv2.rectangle(color_image, (int(xyxy[0]), int(xyxy[1])), (int(xyxy[2]), int(xyxy[3])), (255, 0, 0), 2)\n",
    "                cv2.putText(color_image, f'{cls}', (int(xyxy[0]), int(xyxy[1]) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)\n",
    "\n",
    "                # Draw bounding box on the depth image\n",
    "                cv2.rectangle(depth_image, (int(xyxy[0]), int(xyxy[1])), (int(xyxy[2]), int(xyxy[3])), (255, 0, 0), 2)\n",
    "\n",
    "    # Display the color image and depth image\n",
    "    cv2.imshow('Color Image with Object Detection', color_image)\n",
    "    cv2.imshow('Depth Image with Object Detection', depth_image)\n",
    "\n",
    "    # Exit the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "# Stop the camera stream\n",
    "pipeline.stop()\n",
    "\n",
    "# Close all windows\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0e68297",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to C:\\Users\\Prabhu/.cache\\torch\\hub\\master.zip\n",
      "YOLOv5  2023-6-7 Python-3.10.4 torch-2.0.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7015519 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n",
      "YOLOv5  2023-6-7 Python-3.10.4 torch-2.0.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7015519 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'fx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 25>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     52\u001b[0m depth_values \u001b[38;5;241m=\u001b[39m depth_image[\u001b[38;5;28mint\u001b[39m(xyxy[\u001b[38;5;241m1\u001b[39m]):\u001b[38;5;28mint\u001b[39m(xyxy[\u001b[38;5;241m3\u001b[39m]), \u001b[38;5;28mint\u001b[39m(xyxy[\u001b[38;5;241m0\u001b[39m]):\u001b[38;5;28mint\u001b[39m(xyxy[\u001b[38;5;241m2\u001b[39m])]\u001b[38;5;241m.\u001b[39mflatten() \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1000.0\u001b[39m  \u001b[38;5;66;03m# Convert from millimeters to meters\u001b[39;00m\n\u001b[0;32m     53\u001b[0m avg_depth \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(depth_values)\n\u001b[1;32m---> 54\u001b[0m rotation_x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marctan((center_x \u001b[38;5;241m-\u001b[39m width \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m*\u001b[39m avg_depth \u001b[38;5;241m/\u001b[39m \u001b[43mfx\u001b[49m) \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m180\u001b[39m \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39mpi)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Draw bounding box and rotation information on the depth image\u001b[39;00m\n\u001b[0;32m     57\u001b[0m cv2\u001b[38;5;241m.\u001b[39mrectangle(depth_image, (\u001b[38;5;28mint\u001b[39m(xyxy[\u001b[38;5;241m0\u001b[39m]), \u001b[38;5;28mint\u001b[39m(xyxy[\u001b[38;5;241m1\u001b[39m])), (\u001b[38;5;28mint\u001b[39m(xyxy[\u001b[38;5;241m2\u001b[39m]), \u001b[38;5;28mint\u001b[39m(xyxy[\u001b[38;5;241m3\u001b[39m])), (\u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'fx' is not defined"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import pyrealsense2 as rs\n",
    "\n",
    "# Load the YOLOv5 model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = torch.hub.load('ultralytics/yolov5', 'custom', 'best.pt', force_reload=True)\n",
    "model = torch.hub.load('yolov5', 'custom', 'best.pt', source='local')\n",
    "model.to(device).eval()\n",
    "\n",
    "# Set camera parameters\n",
    "width, height = 640, 480  # Set the desired frame size\n",
    "fps = 30  # Set the desired frame rate\n",
    "# Initialize the RealSense camera\n",
    "pipeline = rs.pipeline()\n",
    "config = rs.config()\n",
    "config.enable_stream(rs.stream.color, width, height, rs.format.bgr8, fps)\n",
    "config.enable_stream(rs.stream.depth, width, height, rs.format.z16, fps)\n",
    "\n",
    "# Start the camera stream\n",
    "pipeline.start(config)\n",
    "\n",
    "# Object detection loop\n",
    "while True:\n",
    "    # Wait for a new frame\n",
    "    frames = pipeline.wait_for_frames()\n",
    "    color_frame = frames.get_color_frame()\n",
    "    depth_frame = frames.get_depth_frame()\n",
    "\n",
    "    # Convert the frames to numpy arrays\n",
    "    color_image = np.asanyarray(color_frame.get_data())\n",
    "    depth_image = np.asanyarray(depth_frame.get_data()).astype(np.float32)\n",
    "\n",
    "    # Normalize the depth values\n",
    "    depth_image /= 1000.0  # Convert from millimeters to meters\n",
    "\n",
    "    # Perform object detection\n",
    "    results = model(depth_image)\n",
    "\n",
    "    # Display the results\n",
    "    for result in results.xyxy[0]:\n",
    "        if result is not None:\n",
    "            xyxy = result[:4].tolist()\n",
    "            conf = result[4].item()\n",
    "            cls = int(result[5].item())\n",
    "\n",
    "            # Only calculate rotation and draw bounding box if confidence is greater than 0.5\n",
    "            if conf > 0.5:\n",
    "                # Calculate rotation with respect to x-axis\n",
    "                center_x = (xyxy[0] + xyxy[2]) / 2\n",
    "                depth_values = depth_image[int(xyxy[1]):int(xyxy[3]), int(xyxy[0]):int(xyxy[2])].flatten() / 1000.0  # Convert from millimeters to meters\n",
    "                avg_depth = np.mean(depth_values)\n",
    "                rotation_x = np.arctan((center_x - width / 2) * avg_depth / fx) * (180 / np.pi)\n",
    "\n",
    "                # Draw bounding box and rotation information on the depth image\n",
    "                cv2.rectangle(depth_image, (int(xyxy[0]), int(xyxy[1])), (int(xyxy[2]), int(xyxy[3])), (255, 0, 0), 2)\n",
    "                cv2.putText(depth_image, f'Rotation: {rotation_x:.2f} degrees', (int(xyxy[0]), int(xyxy[1]) - 10),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)\n",
    "\n",
    "    # Display the depth image\n",
    "    cv2.imshow('Depth Image with Object Detection', depth_image)\n",
    "\n",
    "    # Exit the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "# Stop the camera stream\n",
    "pipeline.stop()\n",
    "\n",
    "# Close all windows\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "404f1ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to C:\\Users\\Prabhu/.cache\\torch\\hub\\master.zip\n"
     ]
    },
    {
     "ename": "URLError",
     "evalue": "<urlopen error [Errno 11001] getaddrinfo failed>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\urllib\\request.py:1348\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[1;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[0;32m   1347\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1348\u001b[0m     \u001b[43mh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1349\u001b[0m \u001b[43m              \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhas_header\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTransfer-encoding\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1350\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err: \u001b[38;5;66;03m# timeout error\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\http\\client.py:1282\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[1;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[0;32m   1281\u001b[0m \u001b[38;5;124;03m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1282\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\http\\client.py:1328\u001b[0m, in \u001b[0;36mHTTPConnection._send_request\u001b[1;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[0;32m   1327\u001b[0m     body \u001b[38;5;241m=\u001b[39m _encode(body, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m-> 1328\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendheaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\http\\client.py:1277\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[1;32m-> 1277\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\http\\client.py:1037\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1036\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer[:]\n\u001b[1;32m-> 1037\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1040\u001b[0m \n\u001b[0;32m   1041\u001b[0m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\http\\client.py:975\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    974\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_open:\n\u001b[1;32m--> 975\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    976\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\http\\client.py:1447\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1445\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnect to a host on a given (SSL) port.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1447\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1449\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tunnel_host:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\http\\client.py:941\u001b[0m, in \u001b[0;36mHTTPConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    940\u001b[0m sys\u001b[38;5;241m.\u001b[39maudit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp.client.connect\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mport)\n\u001b[1;32m--> 941\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    942\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource_address\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    943\u001b[0m \u001b[38;5;66;03m# Might fail in OSs that don't implement TCP_NODELAY\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\socket.py:824\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address)\u001b[0m\n\u001b[0;32m    823\u001b[0m err \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 824\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSOCK_STREAM\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    825\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\socket.py:955\u001b[0m, in \u001b[0;36mgetaddrinfo\u001b[1;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[0;32m    954\u001b[0m addrlist \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 955\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43m_socket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfamily\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    956\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "\u001b[1;31mgaierror\u001b[0m: [Errno 11001] getaddrinfo failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mURLError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Load the YOLOv5 model\u001b[39;00m\n\u001b[0;32m      7\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43multralytics/yolov5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcustom\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbest.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_reload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Set camera parameters\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\hub.py:555\u001b[0m, in \u001b[0;36mload\u001b[1;34m(repo_or_dir, model, source, trust_repo, force_reload, verbose, skip_validation, *args, **kwargs)\u001b[0m\n\u001b[0;32m    551\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    552\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnknown source: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msource\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m. Allowed values: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgithub\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m | \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    554\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m source \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgithub\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 555\u001b[0m     repo_or_dir \u001b[38;5;241m=\u001b[39m \u001b[43m_get_cache_or_reload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_or_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_reload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_repo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mload\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    556\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_validation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_validation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    558\u001b[0m model \u001b[38;5;241m=\u001b[39m _load_local(repo_or_dir, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    559\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\hub.py:230\u001b[0m, in \u001b[0;36m_get_cache_or_reload\u001b[1;34m(github, force_reload, trust_repo, calling_fn, verbose, skip_validation)\u001b[0m\n\u001b[0;32m    228\u001b[0m     url \u001b[38;5;241m=\u001b[39m _git_archive_link(repo_owner, repo_name, ref)\n\u001b[0;32m    229\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDownloading: \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(url, cached_file))\n\u001b[1;32m--> 230\u001b[0m     \u001b[43mdownload_url_to_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcached_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m err\u001b[38;5;241m.\u001b[39mcode \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[0;32m    233\u001b[0m         \u001b[38;5;66;03m# Getting a 300 Multiple Choices error likely means that the ref is both a tag and a branch\u001b[39;00m\n\u001b[0;32m    234\u001b[0m         \u001b[38;5;66;03m# in the repo. This can be disambiguated by explicitely using refs/heads/ or refs/tags\u001b[39;00m\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;66;03m# See https://git-scm.com/book/en/v2/Git-Internals-Git-References\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         \u001b[38;5;66;03m# Here, we do the same as git: we throw a warning, and assume the user wanted the branch\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\hub.py:611\u001b[0m, in \u001b[0;36mdownload_url_to_file\u001b[1;34m(url, dst, hash_prefix, progress)\u001b[0m\n\u001b[0;32m    609\u001b[0m file_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    610\u001b[0m req \u001b[38;5;241m=\u001b[39m Request(url, headers\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.hub\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[1;32m--> 611\u001b[0m u \u001b[38;5;241m=\u001b[39m \u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    612\u001b[0m meta \u001b[38;5;241m=\u001b[39m u\u001b[38;5;241m.\u001b[39minfo()\n\u001b[0;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(meta, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgetheaders\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\urllib\\request.py:216\u001b[0m, in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    215\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[1;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\urllib\\request.py:525\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_response\u001b[38;5;241m.\u001b[39mget(protocol, []):\n\u001b[0;32m    524\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[1;32m--> 525\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\urllib\\request.py:634\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[1;34m(self, request, response)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m):\n\u001b[1;32m--> 634\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhdrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\urllib\\request.py:557\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[1;34m(self, proto, *args)\u001b[0m\n\u001b[0;32m    555\u001b[0m     http_err \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    556\u001b[0m args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mdict\u001b[39m, proto, meth_name) \u001b[38;5;241m+\u001b[39m args\n\u001b[1;32m--> 557\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    558\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[0;32m    559\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\urllib\\request.py:496\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[0;32m    495\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[1;32m--> 496\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    497\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    498\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\urllib\\request.py:749\u001b[0m, in \u001b[0;36mHTTPRedirectHandler.http_error_302\u001b[1;34m(self, req, fp, code, msg, headers)\u001b[0m\n\u001b[0;32m    746\u001b[0m fp\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m    747\u001b[0m fp\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m--> 749\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\urllib\\request.py:519\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    516\u001b[0m     req \u001b[38;5;241m=\u001b[39m meth(req)\n\u001b[0;32m    518\u001b[0m sys\u001b[38;5;241m.\u001b[39maudit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murllib.Request\u001b[39m\u001b[38;5;124m'\u001b[39m, req\u001b[38;5;241m.\u001b[39mfull_url, req\u001b[38;5;241m.\u001b[39mdata, req\u001b[38;5;241m.\u001b[39mheaders, req\u001b[38;5;241m.\u001b[39mget_method())\n\u001b[1;32m--> 519\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[38;5;66;03m# post-process response\u001b[39;00m\n\u001b[0;32m    522\u001b[0m meth_name \u001b[38;5;241m=\u001b[39m protocol\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_response\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\urllib\\request.py:536\u001b[0m, in \u001b[0;36mOpenerDirector._open\u001b[1;34m(self, req, data)\u001b[0m\n\u001b[0;32m    533\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m    535\u001b[0m protocol \u001b[38;5;241m=\u001b[39m req\u001b[38;5;241m.\u001b[39mtype\n\u001b[1;32m--> 536\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_open\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[0;32m    537\u001b[0m \u001b[43m                          \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_open\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[0;32m    539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\urllib\\request.py:496\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[0;32m    495\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[1;32m--> 496\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    497\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    498\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\urllib\\request.py:1391\u001b[0m, in \u001b[0;36mHTTPSHandler.https_open\u001b[1;34m(self, req)\u001b[0m\n\u001b[0;32m   1390\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttps_open\u001b[39m(\u001b[38;5;28mself\u001b[39m, req):\n\u001b[1;32m-> 1391\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHTTPSConnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1392\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_hostname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\urllib\\request.py:1351\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[1;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[0;32m   1348\u001b[0m         h\u001b[38;5;241m.\u001b[39mrequest(req\u001b[38;5;241m.\u001b[39mget_method(), req\u001b[38;5;241m.\u001b[39mselector, req\u001b[38;5;241m.\u001b[39mdata, headers,\n\u001b[0;32m   1349\u001b[0m                   encode_chunked\u001b[38;5;241m=\u001b[39mreq\u001b[38;5;241m.\u001b[39mhas_header(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTransfer-encoding\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m   1350\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err: \u001b[38;5;66;03m# timeout error\u001b[39;00m\n\u001b[1;32m-> 1351\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m URLError(err)\n\u001b[0;32m   1352\u001b[0m     r \u001b[38;5;241m=\u001b[39m h\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[0;32m   1353\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "\u001b[1;31mURLError\u001b[0m: <urlopen error [Errno 11001] getaddrinfo failed>"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import pyrealsense2 as rs\n",
    "\n",
    "# Load the YOLOv5 model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = torch.hub.load('ultralytics/yolov5', 'custom', 'best.pt', force_reload=True)\n",
    "model.to(device).eval()\n",
    "\n",
    "# Set camera parameters\n",
    "width, height = 640, 480  # Set the desired frame size\n",
    "fps = 30  # Set the desired frame rate\n",
    "\n",
    "# Initialize the RealSense camera\n",
    "pipeline = rs.pipeline()\n",
    "config = rs.config()\n",
    "config.enable_stream(rs.stream.color, width, height, rs.format.bgr8, fps)\n",
    "config.enable_stream(rs.stream.depth, width, height, rs.format.z16, fps)\n",
    "\n",
    "# Start the camera stream\n",
    "pipeline.start(config)\n",
    "\n",
    "# Object detection loop\n",
    "while True:\n",
    "    # Wait for a new frame\n",
    "    frames = pipeline.wait_for_frames()\n",
    "    color_frame = frames.get_color_frame()\n",
    "    depth_frame = frames.get_depth_frame()\n",
    "\n",
    "    # Convert the frames to numpy arrays\n",
    "    color_image = np.asanyarray(color_frame.get_data())\n",
    "    depth_image = np.asanyarray(depth_frame.get_data()).astype(np.float32)\n",
    "\n",
    "    # Normalize the depth values\n",
    "    depth_image /= 1000.0  # Convert from millimeters to meters\n",
    "\n",
    "    # Perform object detection\n",
    "    results = model(color_image)\n",
    "\n",
    "    # Display the results\n",
    "    for result in results.xyxy[0]:\n",
    "        if result is not None:\n",
    "            xyxy = result[:4].tolist()\n",
    "            conf = result[4].item()\n",
    "            cls = int(result[5].item())\n",
    "\n",
    "            # Only draw bounding box and label if confidence is greater than 0.7\n",
    "            if conf > 0.5:\n",
    "                # Draw bounding box and label on the frame\n",
    "                cv2.rectangle(color_image, (int(xyxy[0]), int(xyxy[1])), (int(xyxy[2]), int(xyxy[3])), (255, 0, 0), 2)\n",
    "                cv2.putText(color_image, f'{conf}', (int(xyxy[0]), int(xyxy[1]) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9,\n",
    "                            (255, 0, 0), 2)\n",
    "\n",
    "                # Get the center of the bounding box\n",
    "                center_x = (xyxy[0] + xyxy[2]) / 2\n",
    "\n",
    "                # Extract the depth values within the bounding box\n",
    "                depth_values = depth_image[int(xyxy[1]):int(xyxy[3]), int(xyxy[0]):int(xyxy[2])].flatten()\n",
    "\n",
    "                # Calculate the average depth\n",
    "                avg_depth = np.mean(depth_values)\n",
    "\n",
    "                # Compute the x rotation\n",
    "                rotation_x = np.arctan((center_x - width / 2) * avg_depth / fx) * (180 / np.pi)\n",
    "\n",
    "                # Draw rotation information on the frame\n",
    "                cv2.putText(color_image, f'Rotation X: {rotation_x:.2f}', (int(xyxy[0]), int(xyxy[1]) - 30),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('Object Detection', color_image)\n",
    "\n",
    "    # Exit the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "# Stop the camera stream\n",
    "pipeline.stop()\n",
    "\n",
    "# Close all windows\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a90b1a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to C:\\Users\\Prabhu/.cache\\torch\\hub\\master.zip\n",
      "YOLOv5  2023-6-7 Python-3.10.4 torch-2.0.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7015519 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n",
      "YOLOv5  2023-6-7 Python-3.10.4 torch-2.0.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7015519 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import pyrealsense2 as rs\n",
    "\n",
    "# Load the YOLOv5 model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = torch.hub.load('ultralytics/yolov5', 'custom', 'best.pt', force_reload=True)\n",
    "model = torch.hub.load('yolov5', 'custom', 'best.pt', source='local')\n",
    "model = model.to(device).eval()\n",
    "\n",
    "# Set camera parameters\n",
    "width, height = 640, 480  # Set the desired frame size\n",
    "fps = 30  # Set the desired frame rate\n",
    "\n",
    "# Camera intrinsic parameters (example values)\n",
    "fx = 421.0  # Focal length in x-axis\n",
    "fy = 421.0  # Focal length in y-axis\n",
    "cx = 427.0  # Principal point x-coordinate\n",
    "cy = 236.0  # Principal point y-coordinate\n",
    "\n",
    "# Camera extrinsic parameters (example values)\n",
    "rotation_matrix = np.array([[0.99997, -0.007577, -0.0027762],\n",
    "                            [0.0075674, 0.99997, -0.0034583],\n",
    "                            [0.0028023, 0.0034372, 0.99999]])\n",
    "translation_vector = np.array([0.015133, -4.4939e-05, 0.0001245])\n",
    "\n",
    "# Initialize the RealSense camera\n",
    "pipeline = rs.pipeline()\n",
    "config = rs.config()\n",
    "config.enable_stream(rs.stream.color, width, height, rs.format.bgr8, fps)\n",
    "config.enable_stream(rs.stream.depth, width, height, rs.format.z16, fps)\n",
    "\n",
    "# Start the camera stream\n",
    "pipeline.start(config)\n",
    "\n",
    "# Object detection loop\n",
    "while True:\n",
    "    # Wait for a new frame\n",
    "    frames = pipeline.wait_for_frames()\n",
    "    color_frame = frames.get_color_frame()\n",
    "    depth_frame = frames.get_depth_frame()\n",
    "\n",
    "    # Convert the frames to numpy arrays\n",
    "    color_image = np.asanyarray(color_frame.get_data())\n",
    "    depth_image = np.asanyarray(depth_frame.get_data())\n",
    "\n",
    "    # Perform object detection\n",
    "    results = model(color_image)\n",
    "\n",
    "    # Display the results\n",
    "    for result in results.xyxy[0]:\n",
    "        if result is not None:\n",
    "            xyxy = result[:4].tolist()\n",
    "            conf = result[4].item()\n",
    "            cls = int(result[5].item())\n",
    "\n",
    "            # Only process if confidence is greater than 0.5\n",
    "            if conf > 0.5:\n",
    "                # Draw bounding box and label on the frame\n",
    "                cv2.rectangle(color_image, (int(xyxy[0]), int(xyxy[1])), (int(xyxy[2]), int(xyxy[3])), (255, 0, 0), 2)\n",
    "                cv2.putText(color_image, f'{conf}', (int(xyxy[0]), int(xyxy[1]) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9,\n",
    "                            (255, 0, 0), 2)\n",
    "\n",
    "                # Get the top-left corner point of the bounding box\n",
    "                x, y = int(xyxy[0]), int(xyxy[1])\n",
    "\n",
    "                # Calculate the depth value at the top-left corner\n",
    "                depth = depth_image[y, x] / 1000.0  # Convert from millimeters to meters\n",
    "\n",
    "                # Calculate the camera coordinates (Xc, Yc, Zc)\n",
    "                Xc = (x - cx) * depth / fx\n",
    "                Yc = (y - cy) * depth / fy\n",
    "                Zc = depth\n",
    "\n",
    "                # Calculate the world coordinates (Xw, Yw, Zw) using camera extrinsics\n",
    "                world_coordinates = np.dot(rotation_matrix, np.array([Xc, Yc, Zc])) + translation_vector\n",
    "                Xw, Yw, Zw = world_coordinates\n",
    "\n",
    "                # Draw three perpendicular lines from the top-left corner\n",
    "                length = 50\n",
    "                cv2.line(color_image, (x, y), (x, y - length), (0, 0, 255), 2)  # Red line (vertical)\n",
    "                cv2.line(color_image, (x, y), (x + length, y), (0, 255, 0), 2)  # Green line (horizontal)\n",
    "                cv2.line(color_image, (x, y), (x + length, y - length), (255, 0, 0), 2)  # Blue line (diagonal)\n",
    "\n",
    "                # Display the world coordinates\n",
    "                cv2.putText(color_image, f'X: {Xw:.2f}', (x + 10, y + 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n",
    "                            (255, 255, 255), 2)\n",
    "                cv2.putText(color_image, f'Y: {Yw:.2f}', (x + 10, y + 30), cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n",
    "                            (255, 255, 255), 2)\n",
    "                cv2.putText(color_image, f'Z: {Zw:.2f}', (x + 10, y + 50), cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n",
    "                            (255, 255, 255), 2)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('Object Detection', color_image)\n",
    "\n",
    "    # Exit the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "# Stop the camera stream\n",
    "pipeline.stop()\n",
    "\n",
    "# Close all windows\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533c4deb",
   "metadata": {},
   "source": [
    "cuboid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2ecc581",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to C:\\Users\\Prabhu/.cache\\torch\\hub\\master.zip\n",
      "YOLOv5  2023-6-7 Python-3.10.4 torch-2.0.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7015519 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n",
      "YOLOv5  2023-6-7 Python-3.10.4 torch-2.0.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7015519 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import pyrealsense2 as rs\n",
    "\n",
    "# Load the YOLOv5 model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = torch.hub.load('ultralytics/yolov5', 'custom', 'best.pt', force_reload=True)\n",
    "model = torch.hub.load('yolov5', 'custom', 'best.pt', source='local')\n",
    "\n",
    "model.to(device).eval()\n",
    "\n",
    "# Set camera parameters\n",
    "width, height = 640, 480  # Set the desired frame size\n",
    "fps = 30  # Set the desired frame rate\n",
    "\n",
    "# Initialize the RealSense camera\n",
    "pipeline = rs.pipeline()\n",
    "config = rs.config()\n",
    "config.enable_stream(rs.stream.color, width, height, rs.format.bgr8, fps)\n",
    "\n",
    "# Start the camera stream\n",
    "pipeline.start(config)\n",
    "\n",
    "# Object detection loop\n",
    "while True:\n",
    "    # Wait for a new frame\n",
    "    frames = pipeline.wait_for_frames()\n",
    "    color_frame = frames.get_color_frame()\n",
    "\n",
    "    # Convert the frame to a numpy array\n",
    "    frame = np.asanyarray(color_frame.get_data())\n",
    "\n",
    "    # Perform object detection\n",
    "    results = model(frame)\n",
    "\n",
    "    # Display the results\n",
    "    for result in results.xyxy[0]:\n",
    "        if result is not None:\n",
    "            xyxy = result[:4].tolist()\n",
    "            conf = result[4].item()\n",
    "            cls = int(result[5].item())\n",
    "\n",
    "            # Only draw bounding box and label if confidence is greater than 0.5\n",
    "            if conf > 0.5:\n",
    "                # Draw cuboid around the object\n",
    "                x1, y1, x2, y2 = map(int, xyxy)\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "\n",
    "                # Draw top face of cuboid\n",
    "                cv2.line(frame, (x1, y1), (x2, y1), (255, 0, 0), 2)\n",
    "                cv2.line(frame, (x2, y1), (x2, y2), (255, 0, 0), 2)\n",
    "                cv2.line(frame, (x2, y2), (x1, y2), (255, 0, 0), 2)\n",
    "                cv2.line(frame, (x1, y2), (x1, y1), (255, 0, 0), 2)\n",
    "\n",
    "                # Draw vertical lines of cuboid\n",
    "                cv2.line(frame, (x1, y1), (x1 + 20, y1 - 20), (255, 0, 0), 2)\n",
    "                cv2.line(frame, (x2, y1), (x2 + 20, y1 - 20), (255, 0, 0), 2)\n",
    "                cv2.line(frame, (x2, y2), (x2 + 20, y2 - 20), (255, 0, 0), 2)\n",
    "                cv2.line(frame, (x1, y2), (x1 + 20, y2 - 20), (255, 0, 0), 2)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('Object Detection', frame)\n",
    "\n",
    "    # Exit the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "# Stop the camera stream\n",
    "pipeline.stop()\n",
    "\n",
    "# Close all windows\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccd5942f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to C:\\Users\\Prabhu/.cache\\torch\\hub\\master.zip\n",
      "YOLOv5  2023-6-7 Python-3.10.4 torch-2.0.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7015519 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n",
      "YOLOv5  2023-6-7 Python-3.10.4 torch-2.0.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7015519 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 480 is out of bounds for axis 0 with size 480",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 26>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     53\u001b[0m depth_tl \u001b[38;5;241m=\u001b[39m depth_image[y1, x1]\n\u001b[0;32m     54\u001b[0m depth_tr \u001b[38;5;241m=\u001b[39m depth_image[y1, x2]\n\u001b[1;32m---> 55\u001b[0m depth_br \u001b[38;5;241m=\u001b[39m \u001b[43mdepth_image\u001b[49m\u001b[43m[\u001b[49m\u001b[43my2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     56\u001b[0m depth_bl \u001b[38;5;241m=\u001b[39m depth_image[y2, x1]\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# Draw closed cuboid\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 480 is out of bounds for axis 0 with size 480"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import pyrealsense2 as rs\n",
    "\n",
    "# Load the YOLOv5 model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = torch.hub.load('ultralytics/yolov5', 'custom', 'best.pt', force_reload=True)\n",
    "model = torch.hub.load('yolov5', 'custom', 'best.pt', source='local')\n",
    "model.to(device).eval()\n",
    "\n",
    "# Set camera parameters\n",
    "width, height = 640, 480  # Set the desired frame size\n",
    "fps = 30  # Set the desired frame rate\n",
    "\n",
    "# Initialize the RealSense camera\n",
    "pipeline = rs.pipeline()\n",
    "config = rs.config()\n",
    "config.enable_stream(rs.stream.color, width, height, rs.format.bgr8, fps)\n",
    "config.enable_stream(rs.stream.depth, width, height, rs.format.z16, fps)\n",
    "\n",
    "# Start the camera stream\n",
    "pipeline.start(config)\n",
    "\n",
    "# Object detection loop\n",
    "while True:\n",
    "    # Wait for a new frame\n",
    "    frames = pipeline.wait_for_frames()\n",
    "    color_frame = frames.get_color_frame()\n",
    "    depth_frame = frames.get_depth_frame()\n",
    "\n",
    "    # Convert the frames to numpy arrays\n",
    "    color_image = np.asanyarray(color_frame.get_data())\n",
    "    depth_image = np.asanyarray(depth_frame.get_data())\n",
    "\n",
    "    # Perform object detection\n",
    "    results = model(color_image)\n",
    "\n",
    "    # Display the results\n",
    "    for result in results.xyxy[0]:\n",
    "        if result is not None:\n",
    "            xyxy = result[:4].tolist()\n",
    "            conf = result[4].item()\n",
    "            cls = int(result[5].item())\n",
    "\n",
    "            # Only draw bounding box and label if confidence is greater than 0.5\n",
    "            if conf > 0.5:\n",
    "                # Draw cuboid around the object\n",
    "                x1, y1, x2, y2 = map(int, xyxy)\n",
    "                cv2.rectangle(color_image, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "\n",
    "                # Get the depth values for the corners of the bounding box\n",
    "                depth_tl = depth_image[y1, x1]\n",
    "                depth_tr = depth_image[y1, x2]\n",
    "                depth_br = depth_image[y2, x2]\n",
    "                depth_bl = depth_image[y2, x1]\n",
    "\n",
    "                # Draw closed cuboid\n",
    "                cv2.line(color_image, (x1, y1), (x1, y2), (255, 0, 0), 2)  # Front face\n",
    "                cv2.line(color_image, (x1, y1), (x2, y1), (255, 0, 0), 2)\n",
    "                cv2.line(color_image, (x2, y1), (x2, y2), (255, 0, 0), 2)\n",
    "                cv2.line(color_image, (x2, y2), (x1, y2), (255, 0, 0), 2)\n",
    "                cv2.line(color_image, (x1, y2), (x1, y1), (255, 0, 0), 2)\n",
    "\n",
    "                cv2.line(color_image, (x1, y1), (x1, y1 - depth_tl), (255, 0, 0), 2)  # Front top edge\n",
    "                cv2.line(color_image, (x2, y1), (x2, y1 - depth_tr), (255, 0, 0), 2)\n",
    "                cv2.line(color_image, (x2, y2), (x2, y2 - depth_br), (255, 0, 0), 2)\n",
    "                cv2.line(color_image, (x1, y2), (x1, y2 - depth_bl), (255, 0, 0), 2)\n",
    "\n",
    "                cv2.line(color_image, (x1, y1 - depth_tl), (x2, y1 - depth_tr), (255, 0, 0), 2)  # Top face\n",
    "                cv2.line(color_image, (x2, y1 - depth_tr), (x2, y2 - depth_br), (255, 0, 0), 2)\n",
    "                cv2.line(color_image, (x2, y2 - depth_br), (x1, y2 - depth_bl), (255, 0, 0), 2)\n",
    "                cv2.line(color_image, (x1, y2 - depth_bl), (x1, y1 - depth_tl), (255, 0, 0), 2)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('Object Detection', color_image)\n",
    "\n",
    "    # Exit the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "# Stop the camera stream\n",
    "pipeline.stop()\n",
    "\n",
    "# Close all windows\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d749e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
